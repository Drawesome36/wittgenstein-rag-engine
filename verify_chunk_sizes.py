#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VerificaciÃ³n crÃ­tica: Asegurar que NO existan chunks > 30,000 caracteres
"""
import json
import sys

if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

print("=" * 80)
print("VERIFICACIÃ“N DE TAMAÃ‘OS DE CHUNKS")
print("=" * 80)
print()

# Leer corpus
chunks = []
with open('wittgenstein_corpus_clean.jsonl', 'r', encoding='utf-8') as f:
    for line in f:
        chunks.append(json.loads(line))

print(f"Total de chunks: {len(chunks)}")
print()

# Analizar distribuciÃ³n de tamaÃ±os
sizes = [len(c['content']) for c in chunks]
sizes_sorted = sorted(sizes, reverse=True)

print("DISTRIBUCIÃ“N DE TAMAÃ‘OS:")
print(f"  - MÃ­nimo: {min(sizes):,} chars")
print(f"  - Promedio: {sum(sizes) // len(sizes):,} chars")
print(f"  - Mediana: {sizes_sorted[len(sizes_sorted)//2]:,} chars")
print(f"  - MÃ¡ximo: {max(sizes):,} chars")
print()

# Verificar chunks que exceden lÃ­mites crÃ­ticos
CRITICAL_LIMIT = 30000  # LÃ­mite duro
WARNING_LIMIT = 25000   # LÃ­mite objetivo

critical_chunks = [c for c in chunks if len(c['content']) > CRITICAL_LIMIT]
warning_chunks = [c for c in chunks if WARNING_LIMIT < len(c['content']) <= CRITICAL_LIMIT]

print("VERIFICACIÃ“N DE LÃMITES:")
print(f"  âœ“ Chunks < {WARNING_LIMIT:,} chars: {len([s for s in sizes if s <= WARNING_LIMIT]):,}")
print(f"  âš  Chunks entre {WARNING_LIMIT:,} y {CRITICAL_LIMIT:,} chars: {len(warning_chunks):,}")
print(f"  âœ— Chunks > {CRITICAL_LIMIT:,} chars (BLOCKER): {len(critical_chunks):,}")
print()

if critical_chunks:
    print("=" * 80)
    print("ðŸš¨ BLOCKING ISSUES DETECTADOS - CHUNKS > 30,000 CHARS")
    print("=" * 80)
    for i, chunk in enumerate(critical_chunks[:10], 1):
        size_kb = len(chunk['content']) / 1024
        print(f"\n{i}. {chunk['source_file']}")
        print(f"   TamaÃ±o: {len(chunk['content']):,} chars ({size_kb:.1f} KB)")
        print(f"   ProposiciÃ³n: {chunk['proposition_id']}")
        print(f"   Chunk part: {chunk.get('chunk_part', 'N/A')}")
        print(f"   Preview: {chunk['content'][:80]}...")
    print()
    print(f"âŒ FAILED: {len(critical_chunks)} chunks exceden el lÃ­mite de embedding")
    sys.exit(1)
else:
    print("=" * 80)
    print("âœ… PASSED: No hay chunks > 30,000 caracteres")
    print("=" * 80)
    print()

    # Mostrar top 10 chunks mÃ¡s grandes (que sÃ­ pasaron)
    print("TOP 10 CHUNKS MÃS GRANDES (todos dentro del lÃ­mite):")
    print("-" * 80)

    chunks_sorted_by_size = sorted(chunks, key=lambda x: len(x['content']), reverse=True)
    for i, chunk in enumerate(chunks_sorted_by_size[:10], 1):
        size = len(chunk['content'])
        chunk_part = chunk.get('chunk_part', None)
        part_info = f" [parte {chunk_part}]" if chunk_part else ""
        print(f"{i}. {size:>6,} chars | {chunk['source_file'][:50]}{part_info}")
        print(f"           Prop: {chunk['proposition_id']} | Period: {chunk['period']}")

    print()
    print("ðŸŽ¯ Corpus listo para indexaciÃ³n vectorial")
    print(f"   - Total chunks: {len(chunks):,}")
    print(f"   - Todos bajo el lÃ­mite de 8,192 tokens (~32,000 chars)")
    print(f"   - MÃ¡ximo chunk: {max(sizes):,} chars")
